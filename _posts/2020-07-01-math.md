---
layout: post
title: EWC derivation
date: 2020-07-01 11:12:00-0400
description: EWC posterior derivation for prior-focused methods
---

In the original $$\textbf{Elastic Weight Consolidation}$$ [paper](https://arxiv.org/pdf/1612.00796.pdf), the authors show a neat result saying that to fit a model sequentially on two datasets,  $$\mathcal{D}_1 $$ and $$\mathcal{D}_2$$, you don't need to store data from  $$ \mathcal{D}_1 $$, provided you know the posterior of your parameters after seeing $$ \mathcal{D}_1 $$, i.e. $$ p(\theta \mid \mathcal{D}_1) $$. In math,

$$
\log p(\theta \mid \mathcal{D} ) = \log p(\theta \mid \mathcal{D}_1) + \log p(\mathcal{D}_2 \mid \theta) - \log p(\mathcal{D}_2)
$$


where,  $$\mathcal{D}=\mathcal{D}_1 \cup \mathcal{D}_2$$. This is super cool, because $$ \log p(\theta \mid \mathcal{D} ) $$ is exactly what you wish to compute (a model trained on the whole dataset), and the R.H.S does not require data from $$ \mathcal{D}_1 $$. When I first read the paper This equation seemed somewhat arbitrary, so here is the full derivation.


Starting with Baye's Rule, we have

$$
\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D} \mid \theta) + \log p(\theta) - \log p(\mathcal{D})
$$

Let's expand out  $$\mathcal{D}=\mathcal{D}_1 \cup \mathcal{D}_2$$.

$$
\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D}_1 \cup \mathcal{D}_2 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1 \cup \mathcal{D}_2)
$$

Note that samples making up the dataset are independent, so we can split the first and last terms


$$
\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D}_1 \mid \theta) + \log p(\mathcal{D}_2 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1) - \log p( \mathcal{D}_2)
$$

Almost there. We have the two last terms of our target equation already. Let's move them to the right and see what's left


$$
\log p(\theta \mid \mathcal{D}) = \bigg( \log p(\mathcal{D}_1 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1) \bigg) + \log p(\mathcal{D}_2 \mid \theta) - \log p( \mathcal{D}_2)
$$

As we can see now, the terms in the big parentheses are the Baye's Rule expression of the missing term, so we're done!

$$
\log p(\theta \mid \mathcal{D} ) = \log p(\theta \mid \mathcal{D}_1) + \log p(\mathcal{D}_2 \mid \theta) - \log p(\mathcal{D}_2)
$$

#### Small Comment
While prior based methods like EWC are theoretically sound, in practice they are quite bad. I strongly believe this is because the assumptions we make so that computing $$ p(\theta | \mathcal{D}_1) $$ is tractable are wayy to restrictive. Maybe more flexible posterior inference is a good research direction. More on that in the next blog post!
